{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "* Evaluate blank values and handle as required\n",
        "* Explore correlation and PPS scores\n",
        "\n",
        "## Inputs\n",
        "* outputs/datasets/collection/titanic_passengers.csv\n",
        "\n",
        "## Outputs\n",
        "* Generated cleaned Train and Test sets, saved at outputs/datasets/cleaned\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Set up the Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Collected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"outputs/datasets/collection/titanic_passengers.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 3 variables with missing data: Age, Cabin and Embarked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the previous stage, a y-data profile shows more information about the nature of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=df, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation and Predictive Power Score Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlation and PPS is used to further investigate how the target variable 'fraud' correlates with the other features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\",numeric_only=True)\n",
        "    df_corr_pearson = df.corr(method=\"pearson\",numeric_only=True)\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
        "                  df_corr_spearman = df_corr_spearman, \n",
        "                  pps_matrix = pps_matrix,\n",
        "                  CorrThreshold = 0.4, PPS_Threshold =0.2,\n",
        "                  figsize=(12,10), font_annot=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The heatmaps, above, reveal the following:\n",
        "* There is a moderation monotonic relationship between Fare and the following variables - Pclass (class), SibSp (number of siblings present) and Parch (number of parents present)\n",
        "* There is a moderate linear relationship between Fare and Pclass (class) and between SibSp and Parch. \n",
        "    * This means that 1st class tickets are likely to be more expensive than 2nd and 3rd class tickets.\n",
        "    * The latter of these findings makes intuitive sense. \n",
        "    \n",
        "There are a number of significant conclusions to be drawn from the PPS heatmap.\n",
        "\n",
        "1. Survived has a moderate (0.52) PP score for predicting Sex\n",
        "2. Fare has a very strong (0.9) PPS for predicting Pclass.\n",
        "3. Fare is also a strong (0.74) predictor for Embarked.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is a custom function to display missing data levels in a Dataframe. It is originally from Code Institute's Walkthrough Project: Churnometer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
        "    df_missing_data = (pd.DataFrame(\n",
        "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                   \"DataType\": df.dtypes}\n",
        "                                    )\n",
        "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
        "                          .query(\"PercentageOfDataset > 0\")\n",
        "                          )\n",
        "\n",
        "    return df_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaning Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This project will handle missing data as follows:\n",
        "\n",
        "* Drop `['Cabin', 'PassengerId', 'Ticket', 'Name']`. Ticket and PassengerId are unique IDs and do not contribute to the dataset. Cabin is nearly unique to each passenger and has a large percent (>70%) missing.\n",
        "* Mean Median Imputer `'Age'`. Mean or median (depending on distribution) will be \n",
        "* Categorical Imputer `'Embarked'`. Missing values will be imputed with the value 'Missing'.\n",
        "\n",
        "The pipeline is created incrementally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('drop', DropFeatures(features_to_drop=['Cabin','PassengerId','Ticket','Name']))\n",
        "])\n",
        "\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impute Age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly, it is necessary to establish whether age is normally distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly, a visual inspection via a Q-Q plot and a Histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "age_data = df['Age'].dropna()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(age_data, kde=True)\n",
        "plt.title(\"Histogram of Age\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(age_data, plot=plt)\n",
        "plt.title(\"Q-Q Plot\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These plots show that the shape looks relatively normal but with some deviation, particularly at the lower end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Shapiro-Wilk test can be used to test a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "alpha = 0.05\n",
        "stat, p = stats.shapiro(age_data)\n",
        "\n",
        "print(f\"Shapiro-Wilk test statistic: {stat:.6f}\")\n",
        "print(f\"P-value: {p:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Therefore, Age is not normally distributed so missing values must be imputed using the median values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('drop', DropFeatures(features_to_drop=['Cabin', 'PassengerId', 'Ticket','Name'])),\n",
        "     ('median', MeanMedianImputer(variables=['Age'], imputation_method='median')),\n",
        "])\n",
        "\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impute Embarked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('drop', DropFeatures(features_to_drop=['Cabin', 'PassengerId', 'Ticket','Name'])),\n",
        "     ('median', MeanMedianImputer(variables=['Age'], imputation_method='median')),\n",
        "      ('categorical_imputer', CategoricalImputer(imputation_method='missing',\n",
        "                                                  fill_value='Missing',\n",
        "                                                  variables=['Embarked']) )\n",
        "     \n",
        "])\n",
        "\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit and Transform Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pipeline.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['Survived'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Push Cleaned Data to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/cleaned')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
